# Fine-Tuning Pipeline for Memory-Infused Dialogic AI

A modular pipeline for preparing, embedding, indexing, and fine-tuning dialogue data using LoRA and FAISS-backed memory on resource-constrained systems.

---

## ğŸ“¦ Prerequisites

**Platform:** Linux (tested in Distrobox)

**Minimum Specs:** CPU with 8 threads and 24â€¯GB RAM for training. You can reduce memory usage by adjusting the following values in `train.py`:

* `per_device_train_batch_size`
* `gradient_accumulation_steps`

**Recommended:** No GPU required. With a few tweaks, itâ€™ll even run on a potato.

---

## ğŸ”§ Environment Setup

```bash
distrobox create --name rhizome-dev --image rhizome-devbox
distrobox enter rhizome-dev
```

Install dependencies:

```bash
# Install packages
pip3 install -r requirements.txt

# Download spacy English model
python3 -m spacy download en_core_web_sm
```

---

## ğŸ“‚ Folder Structure

```
AI_Fine_Tuning_Pipeline/
â”œâ”€â”€ PDFs/                   # Raw PDFs to be parsed into text
â”œâ”€â”€ data_finetune/          # Clean Q&A dataset generated for fine-tuning. Created once `data_formatter.py` is ran
â”œâ”€â”€ dialogpt-finetuned/     # Checkpoint outputs from training. Created once `train.py` is ran 
â”œâ”€â”€ conversations.json      # Your exported conversation history. The one here is just a placeholder
â”œâ”€â”€ batch_embedder.py
â”œâ”€â”€ chat.py
â”œâ”€â”€ data_formatter.py
â”œâ”€â”€ embedding_config.json   # Created once `batch_embedder.py` is ran 
â”œâ”€â”€ memory.index            # Created once `batch_embedder.py` is ran 
â”œâ”€â”€ memory_texts.npy        # Created once `batch_embedder.py` is ran 
â”œâ”€â”€ memory_metadata.pkl     # Created once `batch_embedder.py` is ran 
â”œâ”€â”€ pdf_to_json.py
â”œâ”€â”€ rhizome.py
â”œâ”€â”€ train.py
â”œâ”€â”€ README.md
```

---

## ğŸ§± Pipeline Overview

### 1. Convert PDFs

```bash
python3 pdf_to_json.py
```

> Converts each PDF into chunked text with metadata.

---

### 2. Add Chat History

Rename your largest `conversations.json` export from ChatGPT (or other AI logs) and place it in the root folder.

---

### 3. Embed and Index Memory

```bash
python3 batch_embedder.py
```

> Creates semantic memory using FAISS, producing:

- `memory.index`
- `memory_texts.npy`
- `memory_metadata.pkl`

---

### 4. Generate Fine-Tuning Dataset

```bash
python3 data_formatter.py
```

> Cleans, deduplicates, and formats Q\&A pairs into `data_finetune/`.
> 
> If you run into issues, tweak `self.quality_score_threshold`.
> Higher values give fewer examples but better quality.

---

### 5. Train the Model (LoRA)

```bash
python3 train.py
```

> LoRA fine-tuning on DialoGPT. Outputs go into `dialogpt-finetuned/`.

---

### 6. Interact with Memory (Optional)

```bash
python3 rhizome.py
```

> Query the FAISS memory index interactively.  
> âš ï¸ Back up the files named memory or avoid running if you need untouched index for data_formatter.py.

---

### 7. Chat with Your Fine-Tuned Model

```bash
python3 chat.py
```

> Loads latest `checkpoint-*` from `dialogpt-finetuned/` and runs in an interactive loop.

---

## ğŸ§  Features

- Built for CPUs (LoRA + SentenceTransformer)
- Semantic memory recall using FAISS
- Fully reproducible from raw PDFs or chat history
- Modular and interpretable stages
- No reliance on proprietary APIs

---

## ğŸ· Notes

- Works best with well-curated, conversational datasets
- Memory-backed recall enables enhanced introspective evaluation
- Logs and training diagnostics are saved automatically

---

## ğŸ“„ License

This project is licensed under the WTFPL â€“ *Do What the Fuck You Want to Public License*.
See [wtfpl.net](http://www.wtfpl.net/) for more.

---
